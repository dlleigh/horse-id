import os
import pandas as pd
from datetime import datetime
from tqdm import tqdm
import re
import sys

# --- Load Configuration ---
from config_utils import load_config, get_data_root

config = load_config()
DATA_ROOT = get_data_root(config)
IMAGE_DIR = config['paths']['dataset_dir'].format(data_root=DATA_ROOT)
# Reads from the detector's output, writes to the final merged file
INPUT_MANIFEST_FILE = config['paths']['detected_manifest_file'].format(data_root=DATA_ROOT)
OUTPUT_MANIFEST_FILE = config['paths']['merged_manifest_file'].format(data_root=DATA_ROOT)
HORSE_HERDS_FILE = config['paths']['horse_herds_file'].format(data_root=DATA_ROOT)
# Note: Features and similarity matching removed - recurring names require manual review


def load_recurring_names():
    """
    Load the horse herds CSV file (output of parse_horse_herds.py) and identify horses 
    with recurring names (names that end with a number like 'Cowboy 1', 'Cowboy 2').
    Returns a set of base names that have recurring horses.
    """
    if not os.path.exists(HORSE_HERDS_FILE):
        print(f"Warning: Horse herds file not found at {HORSE_HERDS_FILE}")
        print("Please run 'python parse_horse_herds.py' first to generate the horse herds CSV.")
        return set()
    
    try:
        print(f"Loading horse herds file: {HORSE_HERDS_FILE}")
        # Read the CSV file generated by parse_horse_herds.py
        df = pd.read_csv(HORSE_HERDS_FILE)
        
        # Extract all unique horse names
        all_horse_names = df['horse_name'].unique().tolist()
        
        # Find horses with numbered names (e.g., "Cowboy 1", "Sunny 2")
        numbered_horses = []
        for horse_name in all_horse_names:
            # Skip NaN values
            if pd.isna(horse_name):
                continue
            
            # Ensure it's a string
            horse_name = str(horse_name)
            
            # Check if name ends with exactly one number (reject "Storm 1 2" type names)
            parts = horse_name.split()
            if len(parts) >= 2 and parts[-1].isdigit():
                # If only 2 parts, it's valid (name + number)
                if len(parts) == 2:
                    numbered_horses.append(horse_name)
                # If more parts, second-to-last should not be a digit
                elif not parts[-2].isdigit():
                    numbered_horses.append(horse_name)
        
        # Extract base names (everything before the last space and number)
        recurring_base_names = set()
        for horse_name in numbered_horses:
            # Remove the last part (number) to get the base name
            parts = horse_name.split()
            base_name = ' '.join(parts[:-1])
            recurring_base_names.add(base_name)
        
        print(f"Found {len(numbered_horses)} horses with numbered names")
        print(f"Identified {len(recurring_base_names)} recurring base names: {sorted(recurring_base_names)}")
        
        return recurring_base_names
        
    except Exception as e:
        print(f"Error loading horse herds file: {e}")
        return set()


def is_recurring_name(horse_name, recurring_base_names):
    """
    Check if a horse name belongs to a recurring name pattern.
    
    Args:
        horse_name (str): The horse name to check
        recurring_base_names (set): Set of base names that have recurring horses
    
    Returns:
        bool: True if the horse name is part of a recurring pattern
    """
    # Check if the name itself ends with a number (e.g., "Cowboy 1")
    if re.match(r'^.+\s+\d+$', horse_name):
        base_name = re.sub(r'\s+\d+$', '', horse_name)
        return base_name in recurring_base_names
    
    # Check if the base name (without number) is in recurring names
    return horse_name in recurring_base_names


def auto_merge_non_recurring_names(output_df, recurring_base_names):
    """
    Automatically merge horses with non-recurring names without detailed analysis.
    
    Args:
        output_df (DataFrame): The output dataframe to modify
        recurring_base_names (set): Set of base names that have recurring horses
    
    Returns:
        DataFrame: Modified dataframe with auto-merged non-recurring names
    """
    # Filter for only single horses
    df_single = output_df[output_df['num_horses_detected'] == 'SINGLE'].copy()
    
    # Group by normalized horse name  
    grouped_by_name = df_single.groupby('normalized_horse_name')
    
    merge_count = 0
    for name, group in grouped_by_name:
        # Skip if this is a recurring name - it will be handled by detailed analysis
        if is_recurring_name(name, recurring_base_names):
            continue
            
        # Skip if only one message (nothing to merge)
        unique_message_ids = group['message_id'].unique()
        if len(unique_message_ids) <= 1:
            continue
            
        print(f"Auto-merging non-recurring horse '{name}' across {len(unique_message_ids)} messages")
        
        # Get all canonical IDs for this name
        canonical_ids = group['canonical_id'].unique()
        
        if len(canonical_ids) > 1:
            # Choose the lowest ID as the final canonical ID
            final_id = min(canonical_ids)
            ids_to_merge = [cid for cid in canonical_ids if cid != final_id]
            
            # Update all rows with this normalized horse name to use the final canonical ID
            merge_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            rows_to_update_mask = (output_df['normalized_horse_name'] == name) & (output_df['canonical_id'].isin(ids_to_merge))
            
            output_df.loc[rows_to_update_mask, 'canonical_id'] = final_id
            output_df.loc[rows_to_update_mask, 'last_merged_timestamp'] = pd.to_datetime(merge_timestamp)
            
            merge_count += 1
            print(f"  Merged IDs {ids_to_merge} into {final_id}")
    
    if merge_count > 0:
        print(f"Auto-merged {merge_count} non-recurring horse names")
    else:
        print("No non-recurring horse names required merging")
    
    return output_df


# Similarity matching functions removed - recurring names handled manually






def validate_canonical_id_consistency(df):
    """
    Validate that all images with the same canonical_id have the same normalized_horse_name.
    This is a critical data integrity rule for the system.
    
    Args:
        df (pandas.DataFrame): The manifest dataframe to validate
        
    Returns:
        bool: True if validation passes, False if violations found
    """
    print("\n=== Validating canonical_id consistency ===")
    
    # Group by canonical_id and check for multiple normalized_horse_name values
    violations = []
    canonical_groups = df.groupby('canonical_id')
    
    for canonical_id, group in canonical_groups:
        unique_names = group['normalized_horse_name'].unique()
        if len(unique_names) > 1:
            violations.append({
                'canonical_id': canonical_id,
                'names': list(unique_names),
                'count': len(group)
            })
    
    if violations:
        print(f"❌ VALIDATION FAILED: Found {len(violations)} canonical IDs with inconsistent normalized_horse_name values:")
        for violation in violations:
            print(f"  - Canonical ID {violation['canonical_id']} ({violation['count']} images) has names: {violation['names']}")
        print("\nThis violates the critical data integrity rule: All images with the same canonical_id MUST have the same normalized_horse_name.")
        print("Please fix these inconsistencies before proceeding.")
        return False
    else:
        print("✅ Validation passed: All canonical IDs have consistent normalized_horse_name values")
        return True

def main():
    """Merges horse identities in the manifest based on photo similarity."""
    print("Starting horse identity merging process...")
    
    # Load recurring names from horse herds CSV file (output of parse_horse_herds.py)
    recurring_base_names = load_recurring_names()
    
    # INPUT_MANIFEST_FILE is config['paths']['detected_manifest_file']
    # OUTPUT_MANIFEST_FILE is config['paths']['merged_manifest_file'] (this script's output)
    print(f"Reading detected manifest: {INPUT_MANIFEST_FILE}")
    try:
        detected_df = pd.read_csv(INPUT_MANIFEST_FILE, dtype={'message_id': str, 'filename': str})
    except FileNotFoundError:
        print(f"Error: Detected manifest file not found at {INPUT_MANIFEST_FILE}. Cannot proceed.")
        return

    # Note: Pre-extracted features no longer needed - similarity matching removed

    output_df = detected_df.copy()

    # Ensure original_canonical_id exists. It's crucial for initializing canonical_id.
    if 'original_canonical_id' not in output_df.columns:
        print("Error: 'original_canonical_id' column missing. Initializing from 'canonical_id' in detected manifest.")
        # This assumes 'canonical_id' from detected_df is effectively the original if 'original_canonical_id' is missing.
        output_df['original_canonical_id'] = output_df['canonical_id']

    # Initialize 'canonical_id' from 'original_canonical_id' for all rows.
    # Previous merge results will overwrite this for relevant rows.
    output_df['canonical_id'] = output_df['original_canonical_id']
    if 'last_merged_timestamp' not in output_df.columns:
        output_df['last_merged_timestamp'] = pd.NaT
    else:
        output_df['last_merged_timestamp'] = pd.to_datetime(output_df['last_merged_timestamp'], errors='coerce').fillna(pd.NaT)

    # Load previous merge results if the output file exists
    if os.path.exists(OUTPUT_MANIFEST_FILE):
        print(f"Loading previous merge results from: {OUTPUT_MANIFEST_FILE}")
        previous_merges_df = pd.read_csv(OUTPUT_MANIFEST_FILE, dtype={'message_id': str, 'filename': str})

        if not previous_merges_df.empty:
            # Merge previous canonical_id, last_merged_timestamp, normalized_horse_name, status, and num_horses_detected into output_df
            cols_to_carry = ['filename']
            if 'canonical_id' in previous_merges_df.columns: cols_to_carry.append('canonical_id')
            if 'last_merged_timestamp' in previous_merges_df.columns: cols_to_carry.append('last_merged_timestamp')
            if 'normalized_horse_name' in previous_merges_df.columns: cols_to_carry.append('normalized_horse_name')
            if 'status' in previous_merges_df.columns: cols_to_carry.append('status')
            if 'num_horses_detected' in previous_merges_df.columns: cols_to_carry.append('num_horses_detected')
            
            temp_df = pd.merge(output_df,
                               previous_merges_df[cols_to_carry],
                               on='filename',
                               how='left',
                               suffixes=('', '_prev'))
            
            # If a previous canonical_id exists for a file, use it. Otherwise, keep the one from original_canonical_id.
            if 'canonical_id_prev' in temp_df.columns:
                output_df['canonical_id'] = temp_df['canonical_id_prev'].fillna(temp_df['canonical_id'])
            if 'last_merged_timestamp_prev' in temp_df.columns:
                temp_df['last_merged_timestamp_prev'] = pd.to_datetime(temp_df['last_merged_timestamp_prev'], errors='coerce').fillna(pd.NaT)
                output_df['last_merged_timestamp'] = temp_df['last_merged_timestamp_prev'].fillna(temp_df['last_merged_timestamp'])
            # Preserve manually corrected normalized_horse_name values from previous merges
            if 'normalized_horse_name_prev' in temp_df.columns:
                output_df['normalized_horse_name'] = temp_df['normalized_horse_name_prev'].fillna(temp_df['normalized_horse_name'])
            # Preserve manually set status values from previous merges
            if 'status_prev' in temp_df.columns:
                output_df['status'] = temp_df['status_prev'].fillna(temp_df['status'])
            # Preserve manually corrected detection values from previous merges
            if 'num_horses_detected_prev' in temp_df.columns:
                output_df['num_horses_detected'] = temp_df['num_horses_detected_prev'].fillna(temp_df['num_horses_detected'])

    # Ensure correct dtypes after potential merges
    output_df['canonical_id'] = pd.to_numeric(output_df['canonical_id'], errors='coerce').astype('Int64')
    output_df['original_canonical_id'] = pd.to_numeric(output_df['original_canonical_id'], errors='coerce').astype('Int64')
    output_df['last_merged_timestamp'] = pd.to_datetime(output_df['last_merged_timestamp'], errors='coerce').fillna(pd.NaT)
    
    # Auto-merge non-recurring horse names without detailed analysis
    print("\n=== Auto-merging non-recurring horse names ===")
    output_df = auto_merge_non_recurring_names(output_df, recurring_base_names)

    # Track recurring names found for manual review summary
    recurring_names_found = []

    # Filter for only single horses, as multiple/none are ambiguous
    df_single = output_df[output_df['num_horses_detected'] == 'SINGLE'].copy()

    # Group by the normalized horse name
    grouped_by_name = df_single.groupby('normalized_horse_name')
    
    print(f"\n=== Checking for recurring horse names (manual review needed) ===")

    for name, group in tqdm(grouped_by_name, desc="Processing names"):
        # Only check recurring names - non-recurring names were already auto-merged
        if not is_recurring_name(name, recurring_base_names):
            continue  # Skip non-recurring names
        
        # Get unique canonical IDs for this recurring name
        unique_canonical_ids = group['canonical_id'].unique()
        
        if len(unique_canonical_ids) <= 1:
            continue # Only one canonical ID, no manual review needed
        
        # Multiple canonical IDs found for recurring name - needs manual review
        print(f"\n⚠️  MANUAL REVIEW NEEDED: Found recurring horse name '{name}' with {len(unique_canonical_ids)} different canonical IDs")
        
        for canonical_id in unique_canonical_ids:
            canonical_group = group[group['canonical_id'] == canonical_id]
            message_ids = canonical_group['message_id'].unique()
            image_count = len(canonical_group)
            print(f"  - Canonical ID {canonical_id}: {image_count} images from {len(message_ids)} email(s)")
        
        # Add to summary for final output
        recurring_names_found.append({
            'name': name,
            'canonical_ids': sorted(unique_canonical_ids),
            'total_images': len(group)
        })

    # Add manual review summary
    if recurring_names_found:
        print(f"\n⚠️  MANUAL REVIEW SUMMARY - {len(recurring_names_found)} recurring horse names need attention:")
        print("\nUse manage_horses.py to review and merge the following horses if they represent the same animal:")
        for horse in recurring_names_found:
            canonical_ids_str = ', '.join(map(str, horse['canonical_ids']))
            print(f"  - '{horse['name']}' ({horse['total_images']} images) - Canonical IDs: {canonical_ids_str}")
        print(f"\nTo review: streamlit run manage_horses.py")
        print("Select each horse name and use the 'Canonical ID Assignment' tab to merge duplicate identities.")
    else:
        print("\n✅ No recurring horse names found requiring manual review.")

    # Validate the final output before saving
    if not validate_canonical_id_consistency(output_df):
        print("\n❌ Cannot save merged manifest due to validation errors.")
        print("Please review the merge process and fix any inconsistencies.")
        sys.exit(1)
    
    # Save the final, merged manifest
    print(f"\nSaving merged manifest to: {OUTPUT_MANIFEST_FILE}")
    output_df.to_csv(OUTPUT_MANIFEST_FILE, index=False)
    print("Process complete.")


if __name__ == '__main__':
    main()